{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keith/Desktop/NUS/Y2S2/IS3107/Project/ezproperty/db\n",
      "/Users/keith/Desktop/NUS/Y2S2/IS3107/Project/ezproperty\n",
      "Connecting to 'localhost' with user 'root'\n",
      "Database | Using database is3107g6.\n",
      "Pipeline | ml__MultiLayerPerceptron   | Retreiving data from data lake with _id.\n",
      "Pipeline | ml__MultiLayerPerceptron   | Loading 0 documents to data lake.\n",
      "Pipeline | ml__MultiLayerPerceptron   | Load process to data warehouse failed.\n",
      "Connecting to 'localhost' with user 'root'\n",
      "Database | Using database is3107g6.\n",
      "Pipeline | ml__MultiLayerPerceptron   | Retreiving data from data lake with _id.\n",
      "Pipeline | ml__MultiLayerPerceptron   | Loading 0 documents to data lake.\n",
      "Pipeline | ml__MultiLayerPerceptron   | Load process to data warehouse failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "\n",
    "from warehouse import DataWarehouse\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path) \n",
    "from db.etl import MultilayerPerceptronPipeline\n",
    "\n",
    "pipe = MultilayerPerceptronPipeline()\n",
    "schema_name = 'ml__MultiLayerPerceptron'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to 'localhost' with user 'root'\n",
      "Database | Using database is3107g6.\n",
      "Database | Query executed successfully.\n",
      "      district  floorRangeStart  floorRangeEnd  area  transactionDate  resale  \\\n",
      "4995       3.0              6.0           10.0  1.06         1.579772     0.0   \n",
      "4996       3.0             16.0           20.0  0.76         1.579772     0.0   \n",
      "4997       3.0             21.0           25.0  1.16         1.579772     0.0   \n",
      "4998       3.0              1.0            5.0  1.16         1.579772     0.0   \n",
      "4999       3.0             11.0           15.0  1.59         1.579772     0.0   \n",
      "\n",
      "        price  \n",
      "4995  2.84946  \n",
      "4996  2.25270  \n",
      "4997  3.30000  \n",
      "4998  3.05700  \n",
      "4999  4.26430  \n",
      "district           float64\n",
      "floorRangeStart    float64\n",
      "floorRangeEnd      float64\n",
      "area               float64\n",
      "transactionDate    float64\n",
      "resale             float64\n",
      "price              float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "db = DataWarehouse()\n",
    "\n",
    "dataset = db.query('''\n",
    "    SELECT district, floorRangeStart, floorRangeEnd, area, transactionDate, resale, price FROM main__PropertyTransaction\n",
    "    LIMIT 5000\n",
    "''')\n",
    "\n",
    "\n",
    "# Data pre-processing to convert all to float and standardise magnitude\n",
    "df = pd.DataFrame(dataset)\n",
    "for column in df.columns:\n",
    "    if column == \"transactionDate\":\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "        df[column] = (df[column].max() - df[column]) / np.timedelta64(1,'Y')\n",
    "    if column == \"price\": # price is in millions\n",
    "        df[column] = df[column].astype(float) / 1e6\n",
    "    if column == \"area\": # area is in 100 square feet\n",
    "        df[column] = df[column].astype(float) / 100\n",
    "    else:\n",
    "        df[column] = df[column].astype(float)\n",
    "print(df.tail())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 6) (500, 6) (4500,) (500,)\n"
     ]
    }
   ],
   "source": [
    "train = df.sample(frac=0.9,random_state=200)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "X_train, Y_train = train[[column for column in df.columns if column != 'price']], train['price']\n",
    "X_test, Y_test = test[[column for column in df.columns if column != 'price']], test['price']\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                448       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,875\n",
      "Trainable params: 8,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "def build_mlp_model():\n",
    "  model = keras.Sequential()\n",
    "  model.add(Dense(6, activation='relu', input_shape=(6,)))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(1, activation='linear'))\n",
    "  return model\n",
    "\n",
    "mlp_model = build_mlp_model()\n",
    "mlp_model.summary()\n",
    "\n",
    "# Build model\n",
    "# def build_mlp_model():\n",
    "#   model = keras.Sequential()\n",
    "#   model.add(Dense(6, activation='relu', input_shape=(6,)))\n",
    "#   # model.add(Dropout(0.1))\n",
    "#   model.add(Dense(4, activation='relu'))\n",
    "#   # model.add(Dropout(0.1))\n",
    "#   model.add(Dense(4, activation='relu'))\n",
    "#   # model.add(Dropout(0.1))\n",
    "#   model.add(Dense(1, activation='linear'))\n",
    "#   return model\n",
    "\n",
    "# mlp_model = build_mlp_model()\n",
    "# mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae', metrics=['mae'])\n",
    "# mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
    "# mlp_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.9324 - mae: 0.9324\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6959 - mae: 0.6959\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6383 - mae: 0.6383\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6157 - mae: 0.6157\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6039 - mae: 0.6039\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5423 - mae: 0.5423\n",
      "Epoch 7/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4910 - mae: 0.4910\n",
      "Epoch 8/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4874 - mae: 0.4874\n",
      "Epoch 9/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4811 - mae: 0.4811\n",
      "Epoch 10/10\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4798 - mae: 0.4798\n"
     ]
    }
   ],
   "source": [
    "mlp_history = mlp_model.fit(X_train, Y_train, epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4419 - mae: 0.4419\n",
      "(500, 6)\n",
      "test loss is 0.4418659806251526\n",
      "test accuracy is 0.4418659806251526\n"
     ]
    }
   ],
   "source": [
    "loss, acc = mlp_model.evaluate(X_test, Y_test)\n",
    "print(X_test.shape)\n",
    "print(f'test loss is {loss}')\n",
    "print(f'test accuracy is {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline | ml__MultiLayerPerceptron   | Loading 1 documents to data lake.\n"
     ]
    }
   ],
   "source": [
    "def save_model_to_db():\n",
    "\n",
    "    #pickling the model\n",
    "    pickled_model = pickle.dumps(mlp_model)\n",
    "\n",
    "    # creating other attributes\n",
    "    model = [{ \"model\": pickled_model, 'name': \"MLP\", 'created_time': time.time()}]\n",
    "    pipe.dl_loader(model, schema_name)\n",
    "\n",
    "save_model_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_to_db():\n",
    "    json_data = {}\n",
    "\n",
    "    result = pipe.dl_getter('ml__MultiLayerPerceptron')\n",
    "    pickled_model = json_data[result]\n",
    "\n",
    "    return pickle.loads(pickled_model)\n",
    " \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b996d74428cb98fb363d931233fbb38b0ce88b2e0e1a2ea6a636f62c3ef3ff6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
